{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[4]\").getOrCreate()"
      ],
      "metadata": {
        "id": "AQDqEpP4mskM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [3,7,8,15,25,265,30,5,3,7,8]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "rdd1=rdd.map(lambda x:x*x)\n",
        "print(rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUypCz5QpCmi",
        "outputId": "8bc393a4-7db3-4ce1-cad0-690572737a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9, 49, 64, 225, 625, 70225, 900, 25, 9, 49, 64]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def square(x):\n",
        "    return x*x\n",
        "rdd2=rdd.map(square)\n",
        "print(rdd2.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4z66F-ApXPl",
        "outputId": "464d41da-411c-4815-8200-0fd82f5e536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9, 49, 64, 225, 625, 70225, 900, 25, 9, 49, 64]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [3,7,8,15,25,265,30,5,3,7,8]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "rdd1=rdd.filter(lambda x:[x%2!=0])\n",
        "print(rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edz-KRpnp0Mc",
        "outputId": "ca96a936-e413-47fe-b2c6-7b19aed60af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 7, 8, 15, 25, 265, 30, 5, 3, 7, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [3,7,8,15,25,265,30,5,3,7,8]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "rdd1=rdd.flatMap(lambda x:[x-1,x*x])\n",
        "print(rdd1.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_xJB7OAsVvE",
        "outputId": "a1341fc9-861b-4eda-bee7-3fa7b3620707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 9, 6, 49, 7, 64, 14, 225, 24, 625, 264, 70225, 29, 900, 4, 25, 2, 9, 6, 49, 7, 64]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [3,7,8,15,25,265,30,5,3,7,8]\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "distinct_rdd = rdd.distinct()\n",
        "print(distinct_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpzFvxmHssaL",
        "outputId": "1c7933e4-b907-4fd7-9fee-b7e221ec702a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 25, 265, 5, 30, 3, 7, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "walk_rdd= spark.sparkContext.parallelize([(\"Ärun\",2),(\"Balaji\",4),(\"Ärun\",6),(\"Sundar\",5),(\"Balaji\",2)])\n",
        "print(type(walk_rdd))\n",
        "print(walk_rdd.collect())\n",
        "s_list=walk_rdd.sortByKey().collect()\n",
        "for Key,value in s_list:\n",
        "    print(Key,value)\n",
        "print(type(s_list))\n",
        "\n",
        "g_list=walk_rdd.groupByKey().collect()\n",
        "for Key,value in g_list:\n",
        "    print(Key,value)\n",
        "print(type(g_list))\n",
        "\n",
        "print(walk_rdd.reduceByKey(lambda x,y :x+y).collect())\n",
        "print(walk_rdd.countByKey().items())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev4BecR6vjxD",
        "outputId": "0de51be7-cefa-4776-8f43-beac84a621cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.rdd.RDD'>\n",
            "[('Ärun', 2), ('Balaji', 4), ('Ärun', 6), ('Sundar', 5), ('Balaji', 2)]\n",
            "Balaji 4\n",
            "Balaji 2\n",
            "Sundar 5\n",
            "Ärun 2\n",
            "Ärun 6\n",
            "<class 'list'>\n",
            "Balaji <pyspark.resultiterable.ResultIterable object at 0x7e941ad22140>\n",
            "Ärun <pyspark.resultiterable.ResultIterable object at 0x7e941acb6b60>\n",
            "Sundar <pyspark.resultiterable.ResultIterable object at 0x7e941acb74c0>\n",
            "<class 'list'>\n",
            "[('Balaji', 6), ('Ärun', 8), ('Sundar', 5)]\n",
            "dict_items([('Ärun', 2), ('Balaji', 2), ('Sundar', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyDDwKGmyGGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/bda.txt'\n"
      ],
      "metadata": {
        "id": "D20-Hxf-eQRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path,'r') as file:\n",
        "  data=file.read()\n",
        "  print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFeKNTK6ej3I",
        "outputId": "8b2787c9-3e5b-4282-bc6d-78c5cf5f9c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"In the development of machine learning algorithms, the optimization of algorithms is crucial for enhancing overall performance. The process of optimization involves adjusting parameters to achieve the best possible results. By using optimization techniques, engineers can improve the efficiency of algorithms and ensure that the algorithms perform optimally. Different optimization methods, such as gradient descent or evolutionary algorithms, can be applied to refine the algorithms and optimize their outputs. Understanding the intricacies of optimization helps in selecting the most suitable algorithms for specific tasks.\"\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, split, count, lit\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Word Count\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "qIo6eNgPji3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/bda.txt'"
      ],
      "metadata": {
        "id": "UzEUEA3Mjsm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd=spark.sparkContext.textFile(file_path)"
      ],
      "metadata": {
        "id": "n1pJqSxLjscl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
        "print(words_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLJtPjkJkXrW",
        "outputId": "6d100562-5bdf-4cb4-8013-aeb067a7fa0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"In', 'the', 'development', 'of', 'machine', 'learning', 'algorithms,', 'the', 'optimization', 'of', 'algorithms', 'is', 'crucial', 'for', 'enhancing', 'overall', 'performance.', 'The', 'process', 'of', 'optimization', 'involves', 'adjusting', 'parameters', 'to', 'achieve', 'the', 'best', 'possible', 'results.', 'By', 'using', 'optimization', 'techniques,', 'engineers', 'can', 'improve', 'the', 'efficiency', 'of', 'algorithms', 'and', 'ensure', 'that', 'the', 'algorithms', 'perform', 'optimally.', 'Different', 'optimization', 'methods,', 'such', 'as', 'gradient', 'descent', 'or', 'evolutionary', 'algorithms,', 'can', 'be', 'applied', 'to', 'refine', 'the', 'algorithms', 'and', 'optimize', 'their', 'outputs.', 'Understanding', 'the', 'intricacies', 'of', 'optimization', 'helps', 'in', 'selecting', 'the', 'most', 'suitable', 'algorithms', 'for', 'specific', 'tasks.\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts=(text_rdd.flatMap(lambda line: line.split())\n",
        "             .map(lambda word: (word, 1))\n",
        "             .reduceByKey(lambda a, b: a + b))\n"
      ],
      "metadata": {
        "id": "mALoTrCbk6Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_counts.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4vZNHkvlURW",
        "outputId": "7fdb8650-98b1-4b80-e59c-b5a609064775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('\"In', 1), ('development', 1), ('of', 5), ('machine', 1), ('learning', 1), ('optimization', 5), ('algorithms', 5), ('is', 1), ('enhancing', 1), ('overall', 1), ('The', 1), ('process', 1), ('involves', 1), ('best', 1), ('possible', 1), ('results.', 1), ('using', 1), ('techniques,', 1), ('engineers', 1), ('improve', 1), ('ensure', 1), ('perform', 1), ('Different', 1), ('methods,', 1), ('as', 1), ('evolutionary', 1), ('applied', 1), ('optimize', 1), ('Understanding', 1), ('in', 1), ('suitable', 1), ('specific', 1), ('the', 8), ('algorithms,', 2), ('crucial', 1), ('for', 2), ('performance.', 1), ('adjusting', 1), ('parameters', 1), ('to', 2), ('achieve', 1), ('By', 1), ('can', 2), ('efficiency', 1), ('and', 2), ('that', 1), ('optimally.', 1), ('such', 1), ('gradient', 1), ('descent', 1), ('or', 1), ('be', 1), ('refine', 1), ('their', 1), ('outputs.', 1), ('intricacies', 1), ('helps', 1), ('selecting', 1), ('most', 1), ('tasks.\"', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word, count in word_counts.collect():\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZTw7BegmEAu",
        "outputId": "4a01331a-67ee-4a40-9a2a-03cbd0462634"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"In: 1\n",
            "development: 1\n",
            "of: 5\n",
            "machine: 1\n",
            "learning: 1\n",
            "optimization: 5\n",
            "algorithms: 5\n",
            "is: 1\n",
            "enhancing: 1\n",
            "overall: 1\n",
            "The: 1\n",
            "process: 1\n",
            "involves: 1\n",
            "best: 1\n",
            "possible: 1\n",
            "results.: 1\n",
            "using: 1\n",
            "techniques,: 1\n",
            "engineers: 1\n",
            "improve: 1\n",
            "ensure: 1\n",
            "perform: 1\n",
            "Different: 1\n",
            "methods,: 1\n",
            "as: 1\n",
            "evolutionary: 1\n",
            "applied: 1\n",
            "optimize: 1\n",
            "Understanding: 1\n",
            "in: 1\n",
            "suitable: 1\n",
            "specific: 1\n",
            "the: 8\n",
            "algorithms,: 2\n",
            "crucial: 1\n",
            "for: 2\n",
            "performance.: 1\n",
            "adjusting: 1\n",
            "parameters: 1\n",
            "to: 2\n",
            "achieve: 1\n",
            "By: 1\n",
            "can: 2\n",
            "efficiency: 1\n",
            "and: 2\n",
            "that: 1\n",
            "optimally.: 1\n",
            "such: 1\n",
            "gradient: 1\n",
            "descent: 1\n",
            "or: 1\n",
            "be: 1\n",
            "refine: 1\n",
            "their: 1\n",
            "outputs.: 1\n",
            "intricacies: 1\n",
            "helps: 1\n",
            "selecting: 1\n",
            "most: 1\n",
            "tasks.\": 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path1= '/content/drive/MyDrive/bda.txt'\n",
        "file_path2= '/content/drive/MyDrive/bda1.txt'"
      ],
      "metadata": {
        "id": "-XVvUcv8oPTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_counts(spark_file_path):\n",
        "    text_rdd=spark.sparkContext.textFile(spark_file_path)\n",
        "    word_counts=(text_rdd.flatMap(lambda line: line.split())\n",
        "             .map(lambda word: (word, 1))\n",
        "             .reduceByKey(lambda a, b: a + b))\n",
        "    return word_counts\n",
        "    def main(file_path1,file_path2):\n",
        "      spark = SparkSession.builder.appName(\"Word Count\").getOrCreate()\n",
        "      word_counts1 = word_counts(file_path1)\n",
        "      word_counts2 = word_counts(file_path2)\n",
        "      combined_counts = word_counts1.union(word_counts2)\n",
        "      for word, count in combined_counts.collect():\n",
        "          print(f\"{word}: {count}\")\n",
        "      spark.stop()"
      ],
      "metadata": {
        "id": "mcVe0kbXsjAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NTjMHzpV0dw1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}